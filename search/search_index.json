{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to AI Functional Catalog","text":"<p>AI FunCat (or func-ai) is a library to help you build a catalog of reusable functions and interact with them using LLMs  (for now only OpenAI is supported)</p>"},{"location":"function-indexing/overview/","title":"Function Indexing","text":"<p>The library supports function indexing (with some limitations). This means that you can index your functions and then query them using the <code>func-ai</code> library. This is useful if you want to query your functions using natural language and especially when you have a lot of functions which cannot fit in LLM context.</p> <p>The Function Indexer (FI) relies on chromadb vector store to store function descriptions and then perform semantic search on those descriptions to find the most relevant functions.</p> <p>Limitations:</p> <ul> <li>partials while supported for indexing and function wrapping using <code>OpenAIFunctionWrapper</code> cannot be rehydrated in the   index once it is reloaded (e.g. app restart). The suggested workaround is at app/script startup to reindex the   partials which will not re-add them in the index but will only rehydrate them in the index map.</li> </ul>"},{"location":"function-indexing/overview/#usage","title":"Usage","text":"<pre><code>import chromadb\nfrom chromadb import Settings\nfrom dotenv import load_dotenv\nfrom func_ai.function_indexer import FunctionIndexer\ndef function_to_index(a: int, b: int) -&gt; int:\n\"\"\"\n    This is a function that adds two numbers\n    :param a: First number\n    :param b: Second number\n    :return: Sum of a and b\n    \"\"\"\nreturn a + b\ndef another_function_to_index() -&gt; str:\n\"\"\"\n    This is a function returns hello world\n    :return: Hello World\n    \"\"\"\nreturn \"Hello World\"\ndef test_function_indexer_init_no_args_find_function_enhanced_summary():\nload_dotenv()\n_indexer = FunctionIndexer(chroma_client=chromadb.PersistentClient(settings=Settings(allow_reset=True)))\n_indexer.reset_function_index()\n_indexer.index_functions([function_to_index, another_function_to_index], enhanced_summary=True)\n_results = _indexer.find_functions(\"Add two numbers\", max_results=10, similarity_threshold=0.2)\nassert len(_results) == 1\nassert _results[0].function(1, 2) == 3\nif __name__ == \"__main__\":\ntest_function_indexer_init_no_args_find_function_enhanced_summary()\n</code></pre> <p>The above code shows how to use the two main functions of the Function Indexer:</p> <ul> <li><code>index_functions</code> which indexes a list of functions</li> <li><code>find_functions</code> which finds functions based on a query string</li> </ul>"},{"location":"function-indexing/overview/#api-docs","title":"API Docs","text":""},{"location":"function-indexing/overview/#functionindexer","title":"FunctionIndexer","text":"<p>Init args:</p> <ul> <li><code>chroma_client</code>: A chromadb client to use for storing the function index. If not provided a new client will be created   using the default settings (e.g. <code>chromadb.PersistentClient(settings=Settings(allow_reset=True))</code>).</li> <li><code>llm_interface</code>: An LLM interface to use for function wrapping. If not provided a new LLM interface will be created   using the default settings (e.g. <code>OpenAIInterface()</code>).</li> <li><code>embedding_function</code>: A function that takes a string and returns an embedding. If not provided the default embedding   function will be used (e.g. <code>embedding_functions.OpenAIEmbeddingFunction()</code>).</li> <li><code>collection_name</code>: The name of the collection to use for storing the function index. If not provided the defaults   to <code>function_index</code>.</li> </ul> <p>Note: You should always initialize your FunctionIndexer with the same embedding function</p>"},{"location":"function-indexing/overview/#index_functions","title":"<code>index_functions</code>","text":"<p>Args:</p> <ul> <li><code>functions</code>: A list of functions to index</li> <li><code>enhanced_summary</code>: If True the function summary will be enhanced with the function docstring. Defaults to False.</li> <li><code>llm_interface</code>: An LLM interface to use for function wrapping. If not provided the one used in Indexer init will be   used</li> </ul>"},{"location":"function-indexing/overview/#find_functions","title":"<code>find_functions</code>","text":"<p>Args:</p> <ul> <li><code>query</code>: The query string to use for finding functions</li> <li><code>max_results</code>: The maximum number of results to return. Defaults to 2.</li> <li><code>similarity_threshold</code>: The similarity threshold to use for filtering results. Defaults to 1.0.</li> </ul> <p>Returns a named tuple <code>SearchResult</code> with the following fields:</p> <ul> <li><code>function</code>: The function actual function that can be directly called</li> <li><code>name</code>: The function name</li> <li><code>wrapper</code>: The <code>OpenAIFunctionWrapper</code> function wrapper</li> <li><code>distance</code>: The distance of the function from the query string</li> </ul> <p>Note: The returned list is sorted by distance in ascending order (e.i. the first result is the closest to the query)</p>"},{"location":"function-indexing/overview/#functions_summary","title":"<code>functions_summary</code>","text":"<p>Returns: A dictionary containing function names and their descriptions.</p>"},{"location":"function-indexing/overview/#index_wrapper_functions","title":"<code>index_wrapper_functions</code>","text":"<p>This identical to <code>index_functions</code> but the list of functions is a list of <code>OpenAIFunctionWrapper</code> objects.</p>"},{"location":"python-functions/python-function-calling/","title":"Python Function Calling","text":"<p>In this article we'll cover how to call Python functions using <code>func-ai</code>.</p>"},{"location":"python-functions/python-function-calling/#pre-requisites","title":"Pre-requisites","text":"<p>Before you begin make sure you have the following:</p> <ul> <li><code>func-ai</code> installed (<code>pip install func-ai</code>)</li> <li>An OpenAI API key set in the <code>OPENAI_API_KEY</code> environment variable (you can have a <code>.env</code> file in the current   directory with <code>OPENAI_API_KEY=&lt;your-api-key&gt;</code> and then use load_dotenv() to load the environment variables from the   file)</li> </ul>"},{"location":"python-functions/python-function-calling/#calling-python-functions-using-openai-api","title":"Calling Python functions using OpenAI API","text":"<p>First let's define a python function we want to call using LLM:</p> <pre><code>def add_two_numbers(a: int, b: int) -&gt; int:\n\"\"\"\n    Adds two numbers\n    :param a: The first number\n    :param b: The second number\n    :return: The sum of the two numbers\n    \"\"\"\nreturn a + b\n</code></pre> <p>A few key points about how functions we want to expose to LLMs should be defined:</p> <ul> <li>The function MUST have type-hints for all parameters and the return value. This helps LLMs understand what the   function does and how to call it.</li> <li>The function MUST have a docstring. The docstring and in particular the description is used by the LLM to identify the   function to call.</li> <li>The function docstring MUST contain parameters and their descriptions. This helps LLMs understand what parameters the   function takes and what they are used for.</li> </ul> <p>Now let's convert the above function so that it can be called using OpenAI function calling capability:</p> <pre><code>from func_ai.utils.py_function_parser import func_to_json\n_json_fun = func_to_json(add_two_numbers)\n</code></pre> <p>In the above snippet we use <code>func_to_json</code> to convert the python function to a dictionary that can be passed to OpenAI API.</p> <p>Now let's do some prompting to see how the function can be called:</p> <pre><code>import openai\nimport json\nfrom dotenv import load_dotenv\nload_dotenv()\ndef call_openai(_messages, _functions: list = None):\nif _functions:\n_open_ai_resp = openai.ChatCompletion.create(\nmodel=\"gpt-3.5-turbo-0613\",\nmessages=_messages,\nfunctions=_functions,\nfunction_call=\"auto\",\ntemperature=0.0,\ntop_p=1.0,\nfrequency_penalty=0.0,\npresence_penalty=0.0,\nmax_tokens=256,\n)\nelse:\n_open_ai_resp = openai.ChatCompletion.create(\nmodel=\"gpt-3.5-turbo-0613\",\nmessages=_messages,\ntemperature=0.5,\ntop_p=1.0,\nfrequency_penalty=0.0,\npresence_penalty=0.0,\nmax_tokens=256,\n)\nreturn _open_ai_resp[\"choices\"][0][\"message\"]\n_messages = [{\"role\": \"system\",\n\"content\": \"You are a helpful automation system that helps users to perform a variety of supported tasks.\"},\n{\"role\": \"user\", \"content\": \"I want to add 5 and 10\"}]\n_functions = [_json_fun]\nresponse = call_openai(_messages, _functions)\nif \"function_call\" in response:\n_result = add_two_numbers(**json.loads(response[\"function_call\"][\"arguments\"]))\nprint(f\"Result: {_result}\")\n_function_call_llm_response = {\n\"role\": \"function\",\n\"name\": response[\"function_call\"][\"name\"],\n\"content\": f\"Result: {_result}\",\n}\n_messages.append(_function_call_llm_response)\nprint(call_openai(_messages))\n</code></pre> <p>The above snippet will print the following:</p> <pre><code>Result: 15\n{\n  \"role\": \"assistant\",\n  \"content\": \"The sum of 5 and 10 is 15.\"\n}\n</code></pre> <p>Let's break down the above snippet:</p> <ul> <li>First we define a function <code>call_openai</code> that takes a list of messages and a list of functions to call. The function   uses the <code>openai.ChatCompletion.create</code> API to call OpenAI and get a response.</li> <li>Next we define a list of messages that we want to send to OpenAI. The first message is a system message that describes   what the system does. The second message is a user message that tells the system what the user wants to do.</li> <li>Next we define a list of functions that we want to expose to OpenAI. In this case we only have one function.</li> <li>Next we call the <code>call_openai</code> function with the messages and functions. The response from OpenAI is stored in the   <code>response</code> variable.</li> <li>Next we check if the response contains a <code>function_call</code> key. If it does then we know that OpenAI has called our   function and we can get the result from the <code>function_call</code> key.</li> <li>Next we print the result of the function call.</li> <li>Next we create a new message that contains the result of the function call and append it to the list of messages.</li> <li>Finally we call the <code>call_openai</code> function again with the updated list of messages. This time OpenAI will respond with   a message that contains the result of the function call.</li> </ul> <p>Non-Production Example</p> <p>The above is a naive example of how you can use the <code>func-ai</code> library to convert your python functions and use them with OpenAI. <code>func-ai</code> offer much more advanced mechanisms to help you build a production ready code. Please check other articles in the documentation to learn more or get in touch with us if you need help.</p>"},{"location":"python-functions/python-function-calling/#working-with-functoolspartial","title":"Working with <code>functools.partial</code>","text":"<p>Python <code>functools</code> library offers the ability to create partial functions with some of the parameters already set. This is particularly useful in cases where you have either static parameter you want to configure, sensitive parameter such a secret or a state object (e.g. DB connection) in which case you either cannot or do not want to send that info to OpenAI. <code>partial</code> to the rescue!</p> <p>Let's create a new function called <code>query_db</code> where we want our DB driver to be a fixed parameter and not passed to the LLM:</p> <p>Note: We make the assumption that <code>call_openai</code> function is already defined as per the previous example.</p> <pre><code>from functools import partial\nfrom func_ai.utils.py_function_parser import func_to_json\nimport json\ndef query_db(db_driver: object, query: str) -&gt; str:\n\"\"\"\n    Queries the database\n    :param db_driver: The database driver to use\n    :param query: The query to execute\n    :return: The result of the query\n    \"\"\"\nreturn f\"Querying {db_driver} with query {query}\"\n_partial_fun = partial(query_db, db_driver=\"MySQL\")\n_json_fun = func_to_json(_partial_fun)\n_messages = [{\"role\": \"system\",\n\"content\": \"You are a helpful automation system that helps users to perform a variety of supported tasks.\"},\n{\"role\": \"user\", \"content\": \"Query the db for quarterly sales.\"}]\n_functions = [_json_fun]\nresponse = call_openai(_messages, _functions)\nif \"function_call\" in response:\n_result = _partial_fun(**json.loads(response[\"function_call\"][\"arguments\"]))\nprint(f\"Result: {_result}\")\n_function_call_llm_response = {\n\"role\": \"function\",\n\"name\": response[\"function_call\"][\"name\"],\n\"content\": f\"Result: {_result}\",\n}\n_messages.append(_function_call_llm_response)\nprint(call_openai(_messages))\n</code></pre> <p>The above snippet will print the following:</p> <pre><code>Result: Querying MySQL with query SELECT * FROM sales WHERE date &gt;= '2021-01-01' AND date &lt;= '2021-12-31'\n{\n  \"role\": \"assistant\",\n  \"content\": \"Here are the quarterly sales for the year 2021:\\n\\n1st Quarter: $XXX\\n2nd Quarter: $XXX\\n3rd Quarter: $XXX\\n4th Quarter: $XXX\\n\\nPlease let me know if there's anything else I can assist you with!\"\n}\n</code></pre> <p>The example above is very similar to our previous example except that this time we have fixed the <code>db_driver</code> parameter which gives you that very important security and privacy aspect especially when playing around with LLMs on the open internet.</p>"},{"location":"python-functions/python-function-calling/#function-wrapper","title":"Function Wrapper","text":"<p><code>func-ai</code> also offers a function wrapper that you can use to wrap your functions and expose them to OpenAI. The wrapper takes care of all the heavy lifting for you. Here is a very short example of how you can use the wrapper:</p> <pre><code>from dotenv import load_dotenv\nfrom func_ai.utils import OpenAIFunctionWrapper, OpenAIInterface\nload_dotenv()\ndef say_hello(name: str):\n\"\"\"\n  This is a function that says hello to the user\n  :param name: Name of the person to say hello to\n  :return:\n  \"\"\"\nprint(f\"Hello {name}!\")\n_func_wrap = OpenAIFunctionWrapper.from_python_function(say_hello, OpenAIInterface())\n_func_wrap.from_prompt(\"Say hello to John\")\n</code></pre> <p>The above snippet will print the following:</p> <pre><code>Hello John!\n</code></pre> <p>Let's break down the above snippet:</p> <ul> <li>First we import the <code>load_dotenv</code> function from the <code>dotenv</code> library. This is used to load the environment variables   from the <code>.env</code> file.</li> <li>Next we import the <code>OpenAIFunctionWrapper</code> and <code>OpenAIInterface</code> classes from the <code>func_ai.utils</code> module.</li> <li>Next we define a function called <code>say_hello</code> that takes a <code>name</code> parameter and prints <code>Hello {name}!</code> to the console.</li> <li>Next we create an instance of the <code>OpenAIFunctionWrapper</code> class by calling the <code>from_python_function</code> method and   passing in the <code>say_hello</code> function and an instance of the <code>OpenAIInterface</code> class.</li> <li>Finally we call the <code>from_prompt</code> method on the <code>OpenAIFunctionWrapper</code> instance and pass in the prompt that we want to   send to OpenAI.</li> </ul> <p>It is also possible to use partials with the wrapper like so:</p> <pre><code>from functools import partial\n_func_wrap = OpenAIFunctionWrapper.from_python_function(partial(say_hello,name=\"World\"), OpenAIInterface())\n_func_wrap.from_prompt(\"Say hello\")\n</code></pre> <p>The above snippet will print the following:</p> <pre><code>Hello World!\n</code></pre> <p>Further Examples<p>For more examples check jupyter notebooks in the <code>tests/jupyter/</code> folder.</p> </p>"}]}